{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#***VITON-pre-processing***"
      ],
      "metadata": {
        "id": "jtbjGsaVN03n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***In this Colab notebook, I attempted to prepare VITON pre-processing which includes human-parsing, densepose pose-estimation, and cloth mask. To achieve this, I utilized various Git repositories, which you can refer to using the following links. If you're running the code on Google Colab, you won't need to install, migrate, or troubleshoot any issues. However, if you want to run the code locally, you may need to install some dependencies. Please note that this Colab notebook is simple and the first version of the process, which will be updated in the future. For reference, you may want to visit the following links:***\n",
        "\n",
        "\n",
        "*   **Dense-pose:** https://github.com/facebookresearch/detectron2\n",
        "*   **Human-parse:** https://github.com/GoGoDuck912/Self-Correction-Human-Parsing\n",
        "* **pose-estimation** https://github.com/CMU-Perceptual-Computing-Lab/openpose *(although in this Colab, I used a PyTorch version of OpenPose due to faster execution time)*\n",
        "* **Cloth-mask:** I developed two solutions, one using OpenCV and the other using U-2-net.\n",
        "\n",
        "‚ùóAlthough I have utilized numerous git repositories, I haven't been able to devote sufficient time to cleaning the code. However, despite the lack of cleanup, the code still functions properly."
      ],
      "metadata": {
        "id": "-BpsEBDnCrI8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyRb3r8Ki2ia",
        "outputId": "3dbaf155-14a5-4e53-bd17-f2db692ce09b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbHzJ3Nyi4Xa",
        "outputId": "a664a085-7639-448d-8334-dc0e68d30f32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/VITON-pre-processing\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/VITON-pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**All of the methods presented in this collaborative notebook are easy to comprehend and are specifically designed for practical usage. To utilize any of the pre-processing steps, you will need to upload your data to a folder titled 'input-folder', execute the script, and observe the outcomes in a folder named 'output-folder'.**"
      ],
      "metadata": {
        "id": "97vgYUVawmY9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Installing dependencies"
      ],
      "metadata": {
        "id": "a5gB8r_QLTmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install -q pyyaml==5.1\n",
        "!pip install -q av\n",
        "!pip install -q fvcore\n",
        "!pip install -q omegaconf\n",
        "!pip install -q ninja"
      ],
      "metadata": {
        "id": "qAeCDWlJzpF1"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zXy4kkJrHz-"
      },
      "source": [
        "# Cloth mask(edge)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZBOBwcOCF38"
      },
      "source": [
        "## using-cv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnkqFMW5AI6_",
        "outputId": "aac13dd8-2324-45c6-9327-fdc3e62b5379"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/VITON-pre-processing/cloth-binary-mask/using-cv\n"
          ]
        }
      ],
      "source": [
        "%cd cloth-binary-mask/using-cv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y06MKVu64dLp"
      },
      "source": [
        "The goal of this code is to process images in an input folder by extracting the cloth regions from the images and saving them as binary mask images in an output folder using OpenCV (cv2) library in Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwikZC24q5yo",
        "outputId": "84ddd992-a261-48f7-d5b1-4ef1e31c9872"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloth mask saved at: output-folder/008320_1.jpg\n",
            "Cloth mask saved at: output-folder/012187_1.jpg\n",
            "Cloth mask saved at: output-folder/015044_1.jpg\n",
            "Cloth mask saved at: output-folder/017039_1.jpg\n",
            "Execution time: 0.0570 seconds\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Record the start time\n",
        "start_time = time.time()\n",
        "\n",
        "def get_cloth_mask(image_path):\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "\n",
        "    _, thresh = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "\n",
        "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    mask = np.zeros_like(image)\n",
        "\n",
        "    cv2.drawContours(mask, contours, -1, (255, 255, 255), -1)\n",
        "\n",
        "    return mask\n",
        "\n",
        "def process_images(input_folder, output_folder):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    image_files = os.listdir(input_folder)\n",
        "    image_files = [f for f in image_files if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))]\n",
        "\n",
        "    for image_file in image_files:\n",
        "        input_path = os.path.join(input_folder, image_file)\n",
        "        output_path = os.path.join(output_folder, image_file)\n",
        "\n",
        "        cloth_mask = get_cloth_mask(input_path)\n",
        "\n",
        "        cv2.imwrite(output_path, cloth_mask)\n",
        "\n",
        "        print(f\"Cloth mask saved at: {output_path}\")\n",
        "\n",
        "input_folder = \"input-folder\"\n",
        "output_folder = \"output-folder\"\n",
        "process_images(input_folder, output_folder)\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the execution time\n",
        "execution_time = end_time - start_time\n",
        "\n",
        "# Print the execution time in seconds\n",
        "print(\"Execution time: {:.4f} seconds\".format(execution_time))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wPl3QT8UuyH",
        "outputId": "990c48c8-e9c6-42e8-f52d-e533439d2f22"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/VITON-pre-processing/cloth-binary-mask\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzcS1rqmCOab"
      },
      "source": [
        "##using U-2-Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDttkZN-CJal",
        "outputId": "84657590-c56b-4ae2-b3e5-63f816e0468a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/VITON-pre-processing/cloth-binary-mask/using-U-2-Net\n"
          ]
        }
      ],
      "source": [
        "%cd using-U-2-Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJDH9kanCmrP",
        "outputId": "ca54bfd7-7fd5-44de-c608-5142f7ca16a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/VITON-pre-processing/cloth-binary-mask/using-U-2-Net/unet\n",
            "...load U2NEP---4.7 MB\n",
            "/content/drive/MyDrive/VITON-pre-processing/cloth-binary-mask/using-U-2-Net\n"
          ]
        }
      ],
      "source": [
        "%cd unet\n",
        "import u2net_load\n",
        "import u2net_run\n",
        "u2net = u2net_load.model(model_name = 'u2netp')\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3SXD-PZC0ZU",
        "outputId": "007cba38-f160-4801-efa4-8d8322255bb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating mask for: Copie de 012187_1.jpg\n",
            "Saving output at output-folder/Copie de 012187_1.jpg\n",
            "Generating mask for: Copie de 017039_1.jpg\n",
            "Saving output at output-folder/Copie de 017039_1.jpg\n",
            "Generating mask for: Copie de 015044_1.jpg\n",
            "Saving output at output-folder/Copie de 015044_1.jpg\n",
            "Generating mask for: Copie de 008320_1.jpg\n",
            "Saving output at output-folder/Copie de 008320_1.jpg\n",
            "Execution time: 0.4704 seconds\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# Record the start time\n",
        "start_time = time.time()\n",
        "u2net_run.infer(u2net, 'input-folder', 'output-folder')\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the execution time\n",
        "execution_time = end_time - start_time\n",
        "\n",
        "# Print the execution time in seconds\n",
        "print(\"Execution time: {:.4f} seconds\".format(execution_time))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ..\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfvMsoHIU06N",
        "outputId": "3aefed65-be09-477a-986f-99d9a24b0c9f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/VITON-pre-processing/cloth-binary-mask\n",
            "/content/drive/MyDrive/VITON-pre-processing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdSnUAXraygu"
      },
      "source": [
        "# Dense pose\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "OpEkBm9j3Wzy"
      },
      "outputs": [],
      "source": [
        "#!git clone https://github.com/facebookresearch/detectron2.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1REJHZbrPm-",
        "outputId": "c3b0912d-ce5a-4330-8476-2c17683c6b34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/VITON-pre-processing/detectron2/projects/DensePose\n"
          ]
        }
      ],
      "source": [
        "%cd detectron2/projects/DensePose"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###densepose using detectron2 "
      ],
      "metadata": {
        "id": "-3hO6Aj1CNqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def generate_densepose(image_path):\n",
        "    # Run DensePose on the input image and generate the dump file\n",
        "    !python apply_net.py dump configs/densepose_rcnn_R_101_FPN_DL_s1x.yaml model_final_844d15.pkl {image_path} --output dump.pkl -v\n",
        "\n",
        "    # Load the DensePose output and process the labels\n",
        "    with open(\"dump.pkl\", \"rb\") as f:\n",
        "        data = pickle.load(f)\n",
        "    im = data[0][\"pred_densepose\"][0].labels.to(\"cpu\")\n",
        "    im = im.reshape(1, 1, im.shape[0], im.shape[1]).to(torch.float32)\n",
        "    im = F.interpolate(im, (256, 192)).reshape(256, 192)\n",
        "    im = im.numpy().astype(np.uint8)\n",
        "\n",
        "    # Save the processed labels as a numpy file\n",
        "    np.save(\"./dump.npy\", im)\n",
        "\n",
        "    # Return the processed labels as a numpy array\n",
        "    return np.load(\"./dump.npy\")"
      ],
      "metadata": {
        "id": "SzBCEnzQ1-6T"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Upload you data in your 'input-folder' also the folder of the result 'output-folder'"
      ],
      "metadata": {
        "id": "wwvgg2iY6P60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_folder = \"input-folder\"\n",
        "output_folder = \"output-folder\"\n",
        "\n",
        "# Create the output folder if it does not exist\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "# Process each image in the input folder\n",
        "for image_file in os.listdir(input_folder):\n",
        "    # Check if the file is an image\n",
        "    if image_file.endswith(\".jpg\") or image_file.endswith(\".jpeg\") or image_file.endswith(\".png\"):\n",
        "        # Construct the full path to the input and output files\n",
        "        input_path = os.path.join(input_folder, image_file)\n",
        "        output_path = os.path.join(output_folder, os.path.splitext(image_file)[0] + \".npy\")\n",
        "\n",
        "        # Generate the DensePose output for the input image\n",
        "        densepose_output = generate_densepose(input_path)\n",
        "\n",
        "        # Save the DensePose output as a numpy file in the output folder\n",
        "        np.save(output_path, densepose_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SdfjcGq2ekj",
        "outputId": "0713c2dc-1aab-4df5-af5b-abfef7bf2289"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m[04/26 11:44:59 apply_net]: \u001b[0mLoading config from configs/densepose_rcnn_R_101_FPN_DL_s1x.yaml\n",
            "\u001b[32m[04/26 11:44:59 apply_net]: \u001b[0mLoading model from model_final_844d15.pkl\n",
            "\u001b[32m[04/26 11:45:01 apply_net]: \u001b[0mLoading data from input-folder/005311_0.jpg\n",
            "/usr/local/lib/python3.9/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "\u001b[32m[04/26 11:45:02 apply_net]: \u001b[0mProcessing input-folder/005311_0.jpg\n",
            "\u001b[32m[04/26 11:45:02 apply_net]: \u001b[0mOutput saved to dump.pkl\n",
            "\u001b[32m[04/26 11:45:10 apply_net]: \u001b[0mLoading config from configs/densepose_rcnn_R_101_FPN_DL_s1x.yaml\n",
            "\u001b[32m[04/26 11:45:10 apply_net]: \u001b[0mLoading model from model_final_844d15.pkl\n",
            "\u001b[32m[04/26 11:45:12 apply_net]: \u001b[0mLoading data from input-folder/009416_0.jpg\n",
            "/usr/local/lib/python3.9/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "\u001b[32m[04/26 11:45:12 apply_net]: \u001b[0mProcessing input-folder/009416_0.jpg\n",
            "\u001b[32m[04/26 11:45:12 apply_net]: \u001b[0mOutput saved to dump.pkl\n",
            "\u001b[32m[04/26 11:45:16 apply_net]: \u001b[0mLoading config from configs/densepose_rcnn_R_101_FPN_DL_s1x.yaml\n",
            "\u001b[32m[04/26 11:45:16 apply_net]: \u001b[0mLoading model from model_final_844d15.pkl\n",
            "\u001b[32m[04/26 11:45:18 apply_net]: \u001b[0mLoading data from input-folder/014835_0.jpg\n",
            "/usr/local/lib/python3.9/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "\u001b[32m[04/26 11:45:19 apply_net]: \u001b[0mProcessing input-folder/014835_0.jpg\n",
            "\u001b[32m[04/26 11:45:19 apply_net]: \u001b[0mOutput saved to dump.pkl\n",
            "\u001b[32m[04/26 11:45:23 apply_net]: \u001b[0mLoading config from configs/densepose_rcnn_R_101_FPN_DL_s1x.yaml\n",
            "\u001b[32m[04/26 11:45:23 apply_net]: \u001b[0mLoading model from model_final_844d15.pkl\n",
            "\u001b[32m[04/26 11:45:26 apply_net]: \u001b[0mLoading data from input-folder/015278_0.jpg\n",
            "/usr/local/lib/python3.9/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "\u001b[32m[04/26 11:45:27 apply_net]: \u001b[0mProcessing input-folder/015278_0.jpg\n",
            "\u001b[32m[04/26 11:45:27 apply_net]: \u001b[0mOutput saved to dump.pkl\n",
            "\u001b[32m[04/26 11:45:30 apply_net]: \u001b[0mLoading config from configs/densepose_rcnn_R_101_FPN_DL_s1x.yaml\n",
            "\u001b[32m[04/26 11:45:30 apply_net]: \u001b[0mLoading model from model_final_844d15.pkl\n",
            "\u001b[32m[04/26 11:45:32 apply_net]: \u001b[0mLoading data from input-folder/018959_0.jpg\n",
            "/usr/local/lib/python3.9/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "\u001b[32m[04/26 11:45:33 apply_net]: \u001b[0mProcessing input-folder/018959_0.jpg\n",
            "\u001b[32m[04/26 11:45:33 apply_net]: \u001b[0mOutput saved to dump.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Comparing the results of the script with the pre-processed VITON dataset"
      ],
      "metadata": {
        "id": "QrSYnmh85688"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This script can be utilized to compare the densepose produced in the VITON dataset with the outcomes obtained from the script, which yield nearly identical results.**\n",
        "\n",
        "\n",
        "***The result will be saved at results folder***\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JYL7emFO5C1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from skimage.color import label2rgb\n",
        "\n",
        "input_folder = \"output-folder\"\n",
        "output_folder = \"results\"\n",
        "\n",
        "# Create output folder if it does not exist\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "# Loop through all .npy files in input folder\n",
        "for file_name in os.listdir(input_folder):\n",
        "    if file_name.endswith(\".npy\"):\n",
        "        # Load labeled image from file\n",
        "        file_path = os.path.join(input_folder, file_name)\n",
        "        pred = np.load(file_path)\n",
        "\n",
        "        # Convert labeled image to RGB image\n",
        "        pred = label2rgb(pred, bg_label=0)\n",
        "\n",
        "        # Convert data type to uint8\n",
        "        pred = (pred * 255).astype(np.uint8)\n",
        "\n",
        "        # Create PIL Image object\n",
        "        pil_pred = Image.fromarray(pred)\n",
        "\n",
        "        # Save image to file\n",
        "        output_path = os.path.join(output_folder, file_name[:-4] + \".png\")\n",
        "        pil_pred.save(output_path)\n"
      ],
      "metadata": {
        "id": "YuuN2wqF43r_"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ..\n",
        "%cd ..\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTeOIm0vVJ_-",
        "outputId": "1dd6dee5-4661-48b7-cef9-51891640184b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/VITON-pre-processing/detectron2/projects\n",
            "/content/drive/MyDrive/VITON-pre-processing/detectron2\n",
            "/content/drive/MyDrive/VITON-pre-processing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Results"
      ],
      "metadata": {
        "id": "evo11EqqGxZi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-processed VITON :\n",
        "\n",
        "![1.png](data:image/png;base64,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)\n",
        "\n",
        "Script result:\n",
        "\n",
        "![2.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "_gvDTYNgG0co"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DfPiGyfyVDTS"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuzXBFnq_po6"
      },
      "source": [
        "#Label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "DykP2R8kHgAX"
      },
      "outputs": [],
      "source": [
        "#git clone https://github.com/GoGoDuck912/Self-Correction-Human-Parsing.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gmN9CKgACz2",
        "outputId": "6acb22a1-01cd-4849-f269-f0984b4b6d95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/VITON-pre-processing/Human-Parsing\n"
          ]
        }
      ],
      "source": [
        "%cd Human-Parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gQQWJdRB7IM",
        "outputId": "e1dad4da-106f-4194-892a-ac1e998a67c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 4/4 [00:00<00:00,  5.81it/s]\n",
            "Execution time: 6.9129 seconds\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "# Record the start time\n",
        "start_time = time.time()\n",
        "!python3 simple_extractor.py --dataset 'lip' --model-restore 'lip_final.pth' --input-dir 'input-folder' --output-dir 'output-folder'\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the execution time\n",
        "execution_time = end_time - start_time\n",
        "\n",
        "# Print the execution time in seconds\n",
        "print(\"Execution time: {:.4f} seconds\".format(execution_time))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlMIDXU_V2EN",
        "outputId": "025f1e01-ad3c-4154-f2fd-35c87fda3c10"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/VITON-pre-processing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjuQMCl8HnFQ"
      },
      "source": [
        "#Pose"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Open-pose for key-points generation"
      ],
      "metadata": {
        "id": "cXbUrDcfRIim"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KkbTCqWHwOV",
        "outputId": "0f9e7aa7-4c39-46c7-f90e-5d9295e4f4ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/VITON-pre-processing/Pose-map\n"
          ]
        }
      ],
      "source": [
        "%cd Pose-map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z23GVLLpIYsk",
        "outputId": "c81e49a8-9444-462c-9628-ccb40d2aa47e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File saved at output-folder/000758_0_keypoints.json\n",
            "File saved at output-folder/015840_0_keypoints.json\n",
            "File saved at output-folder/016073_0_keypoints.json\n",
            "Execution time: 16.4165 seconds\n"
          ]
        }
      ],
      "source": [
        "from predict_pose import generate_pose_keypoints\n",
        "import time\n",
        "# Record the start time\n",
        "start_time = time.time()\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "input_folder = 'input-folder'\n",
        "output_folder = 'output-folder'\n",
        "for filename in os.listdir(input_folder):\n",
        "  if filename.endswith('.jpg') or filename.endswith('.png'):\n",
        "        img_path = os.path.join(input_folder, filename)\n",
        "        img_name = os.path.splitext(filename)[0]\n",
        "        pose_path = os.path.join(output_folder, img_name + '_keypoints.json')\n",
        "        generate_pose_keypoints(img_path, pose_path)\n",
        "\n",
        "\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the execution time\n",
        "execution_time = end_time - start_time\n",
        "\n",
        "# Print the execution time in seconds\n",
        "print(\"Execution time: {:.4f} seconds\".format(execution_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Comparing script results with VITON-pre-processed data"
      ],
      "metadata": {
        "id": "MjjnrZNqROhs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**I used a straightforward approach by assigning a threshold value of 20 to each keypoint. Based on this, I obtained a threshold value of approximately 340, which I then applied to check the similarity between the two methods. I have previously employed this method in other works that required the use of OpenPose, and I achieved similar results to the desired outcome.**"
      ],
      "metadata": {
        "id": "JaOoH7NHRXur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load the two JSON files\n",
        "with open('output-folder/000758_0_keypoints.json') as f:\n",
        "    data1 = json.load(f)\n",
        "\n",
        "with open('output-folder/000758_0_keypoints (1).json') as f:#it's from the key-points of the pre-processed data\n",
        "    data2 = json.load(f)\n",
        "\n",
        "# Get the pose keypoints from each file\n",
        "keypoints1 = data1['people'][0]['pose_keypoints']\n",
        "keypoints2 = data2['people'][0]['pose_keypoints']\n",
        "\n",
        "# Compare the keypoints in each file\n",
        "for i in range(len(keypoints1)):\n",
        "    if abs(keypoints1[i] - keypoints2[i]) > 340:   # set a threshold for the difference\n",
        "        print(\"The two files have different pose keypoints.\")\n",
        "        break\n",
        "else:\n",
        "    print(\"The two files have almost the same pose keypoints.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhUQucoyO5Kf",
        "outputId": "c4325efa-ed0e-4217-9c6b-d67021e901b5"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The two files have almost the same pose keypoints.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "nZBOBwcOCF38",
        "NzcS1rqmCOab"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}